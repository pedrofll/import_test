"""
Scraper de Diagn√≥stico para El Corte Ingl√©s
OBJETIVO: Ver qu√© demonios nos devuelve Google Cache (HTML, JSON o Error).
"""

import re
import time
import random
import urllib.parse
from bs4 import BeautifulSoup
import warnings

warnings.filterwarnings("ignore")

try:
    from curl_cffi import requests
    print("‚úÖ curl_cffi cargado correctamente (Modo Camuflaje).")
    USAR_CURL_CFFI = True
except ImportError:
    import requests
    print("‚ö†Ô∏è curl_cffi no instalado. Usando requests est√°ndar.")
    USAR_CURL_CFFI = False

# =========================
# CONFIGURACI√ìN
# =========================

# Probamos la P√°gina 1 (que suele estar cacheada) y la P√°gina 2 (que suele fallar)
URLS_PRUEBA = [
    "https://www.elcorteingles.es/electronica/moviles-y-smartphones/",
    "https://www.elcorteingles.es/electronica/moviles-y-smartphones/2/"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

def mask_url(u: str) -> str:
    try:
        p = urllib.parse.urlparse(u)
        return urllib.parse.urlunparse((p.scheme, p.netloc, p.path, "", "", ""))
    except: return u

def analizar_html(html: str, fuente: str):
    print(f"\n   üî¨ ANALIZANDO CONTENIDO DE {fuente}...")
    
    if not html:
        print("      ‚ùå El HTML est√° vac√≠o.")
        return

    soup = BeautifulSoup(html, "html.parser")
    
    # 1. T√≠tulo de la p√°gina
    titulo = soup.title.string.strip() if soup.title else "SIN T√çTULO"
    print(f"      üè∑Ô∏è  T√≠tulo: '{titulo}'")
    print(f"      üìè Longitud: {len(html)} caracteres")
    
    # 2. Detecci√≥n de errores comunes
    if "404" in titulo or "No hay cach√©" in html:
        print("      ‚õî DIAGN√ìSTICO: P√°gina no encontrada en Google Cache (404).")
        return
    if "robot" in html.lower() or "captcha" in html.lower():
        print("      ‚õî DIAGN√ìSTICO: Bloqueo de Google (Captcha).")
        return

    # 3. B√∫squeda de JSON de productos
    # Buscamos la palabra clave "brand":"Samsung" o "brand":"Apple"
    print("      üîç Buscando huellas de productos...")
    
    if 'brand":"Samsung"' in html:
        print("      ‚úÖ ¬°EUREKA! Se detectaron datos de SAMSUNG en el c√≥digo.")
    elif 'brand":"Apple"' in html:
        print("      ‚úÖ ¬°EUREKA! Se detectaron datos de APPLE en el c√≥digo.")
    else:
        print("      ‚ö†Ô∏è  No veo marcas conocidas en el texto plano.")

    # 4. Extracci√≥n de muestra de JSON
    # Intentamos sacar un trocito de texto que parezca JSON para ver el formato
    # Buscamos algo que empiece por {"id" o {"brand"
    match = re.search(r'\{"brand":"[^"]+".*?"price":\{.*?\}', html)
    if match:
        print(f"      üìù Muestra de JSON encontrado:\n      {match.group(0)[:200]}...")
    else:
        # Si falla el regex anterior, probamos uno m√°s simple
        print("      ‚ö†Ô∏è  Regex estricta fall√≥. Buscando cualquier estructura JSON...")
        match_simple = re.search(r'data-json="([^"]+)"', html)
        if match_simple:
            print(f"      üìù Encontrado atributo data-json (HTML encoding):\n      {match_simple.group(1)[:100]}...")
        else:
            print("      ‚ùå NO SE ENCUENTRA NING√öN JSON RECONOCIBLE.")
            # Imprimimos un trozo del body para ver qu√© hay
            body_text = soup.get_text()[:300].replace("\n", " ")
            print(f"      üìÑ Texto visible (inicio): {body_text}")

def fetch_google_cache(url: str):
    session = requests.Session(impersonate="chrome110") if USAR_CURL_CFFI else requests.Session()
    session.headers.update(HEADERS)
    
    clean_url = url.split("?")[0]
    # Probamos sin strip para ver todo el c√≥digo
    cache_link = f"http://webcache.googleusercontent.com/search?q=cache:{urllib.parse.quote(clean_url)}&strip=0&vwsrc=0"
    
    print(f"üåç Conectando a Google Cache: {mask_url(url)}")
    try:
        r = session.get(cache_link, timeout=20, verify=False)
        print(f"   Estado HTTP: {r.status_code}")
        if r.status_code == 200:
            analizar_html(r.text, "GOOGLE CACHE")
        elif r.status_code == 429:
            print("   ‚õî Google nos est√° bloqueando (Too Many Requests).")
    except Exception as e:
        print(f"   ‚ùå Error de conexi√≥n: {e}")

def main():
    print("--- INICIANDO DIAGN√ìSTICO ---")
    for url in URLS_PRUEBA:
        print("-" * 60)
        fetch_google_cache(url)
        time.sleep(3)
    print("-" * 60)

if __name__ == "__main__":
    main()
